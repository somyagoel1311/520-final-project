# 520-final-project
520 final project

## Tool Comparison : Neptune AI vs WanDB AI

### Model Integration:

#### 1. Neptune 

Steps to Integrate your ML model to Neptune AI:

1. Run the script 'neptune_integration.py'.
2. It'll prompt you for an API token. This token can be accessed from your Neptune account. For the purpose of this project, we will provide this token

Steps to find your NEPTUNE API token:

1. There will be a user menu at the bottom left of your Neptune app. Expand it and click on "Get your API token." That will display your API token. Copy it to your clipboard, and enter it when promopted.

#### 2. WanDB AI 

Steps to Initiate WandB logging to the ML model - 

1. On Running the wandb script, you will recieve a prompt for authorising with an API Token on https://wandb.ai/authorize. To run this smoothly we have provided the API token. 
2. It can also be found in The WandB API Token is found in the "User settings" Section. 

Wandb results Report Link - 
[Published report of results](https://api.wandb.ai/links/ssmm/pj7nk9y3)

#### 3. Evaluation

##### 3.1 Evaluation metrics
For an effective study and analysis we have identified a set of functional and non-functional requirements that are essential for MLOps tools. We have studied features encapsulating these requirements and are quantitatively measuring them using a Rating scale described in detail below.

Functional Requirements:

1. Dataset versioning -> Versioning of data in different stages and experiments for reproducibility
2. Model versioning -> Versioning of models to help in comparison, running multiple experiments, etc
3. Collaboration ->  Features supporting multi-member team collaboration on projects.
4. Framework Integration -> Integration of popular ML frameworks in the market
5. Report generation and management -> Creating and managing of reports for cataloging and stakeholder communication.
6. Model Visualization -> Visualization of the model for explainability

Non Functional Requirements:

1. Understandability -> The documentation and resources available to learn and onboard with the tools.
2. Usability -> The ease of onboarding and working with the tool day to day.
3. Debuggability -> Capabilities supporting the debugging of issues as part of any experiment/run.


##### 3.2 Rating scale

We are using a 3 point quantitative scale to evaluate how well a tool works. For each of our mentioned evaluation criterias,
both the tools get a rating between 1 and 5. Following is the definition of each rating:

1. Tool does not have this ability.
2. Tool has a satisfactory performance for this evaluation criteria.
3. Tool performs well for this criteria and also makes it easy to use this ability.

Results - 

The screenshots used for comaprsion added to the folder named results. 


